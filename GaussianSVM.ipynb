{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning - LabWork-2 - Mercan Karacabey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BDA502 SPRING 2018-19\\n\\n- Gaussian Naive Bayes\\n- Support Vector Machines\\n\\nTASK-1: Please explain how the Naive Bayesian algorithm work very briefly by using the example below.\\n\\nTASK-2: Please use the small dataset about gender classification with Gaussian Naive Bayes.\\n\\nTASK-3: Please use the digits dataset with comparing Gaussian Naive Bayes and Support Vector Machine algorithms. \\n\\nTASK-4: Illustrate a linear and non-linear SVM model with plots. You need to find a suitable example, run it and explain it.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"BDA502 SPRING 2018-19\n",
    "\n",
    "- Gaussian Naive Bayes\n",
    "- Support Vector Machines\n",
    "\n",
    "TASK-1: Please explain how the Naive Bayesian algorithm work very briefly by using the example below.\n",
    "\n",
    "TASK-2: Please use the small dataset about gender classification with Gaussian Naive Bayes.\n",
    "\n",
    "TASK-3: Please use the digits dataset with comparing Gaussian Naive Bayes and Support Vector Machine algorithms. \n",
    "\n",
    "TASK-4: Illustrate a linear and non-linear SVM model with plots. You need to find a suitable example, run it and explain it.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK-1: Simple Example for GaussianNB from scikit-learn\n",
    " x -> features\n",
    " y -> labels\n",
    "\n",
    " GaussianNB algorithm --  Classification Algorithm\n",
    " We have two sets. 1 and 2 . Then we want to show that new value is which set. fit function -- run the gaussian method for A dataset and B sets.\n",
    " \n",
    " 1- Class prior value - Which set is appropriate for new values? And close which set.\n",
    " 2- Class Count value - During the training step , how much class was generated?\n",
    " 3- Score function - Algorithm and training result for new values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, I calculated time difference of fit the algorithm GaussianNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0019979476928710938\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 1, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB(priors=None)\n",
    "clf.fit(X, Y) # GaussianNB(priors=None)\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time difference : 0.0019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB is a classification algorithm. Calculate probability value and decide which set is appropriate for value depends on probability value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0.66666667 0.33333333]\n",
      "[4. 2.]\n",
      "[[-1.25 -0.75]\n",
      " [ 2.5   1.5 ]]\n",
      "[[2.1875 1.1875]\n",
      " [0.25   0.25  ]]\n",
      "[[0.94580782 0.05419218]]\n",
      "[[-0.05571588 -2.91521869]]\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([[-0.8, -1]]))\n",
    "print(clf.class_prior_)  # probability of each class.\n",
    "print(clf.class_count_)  # number of training samples observed in each class.\n",
    "print(clf.theta_)  # mean of each feature per class.\n",
    "print(clf.sigma_)  # variance of each feature per class\n",
    "print(clf.predict_proba([[0.8, 1]]))  # Return probability estimates for the test vector X.\n",
    "print(clf.predict_log_proba([[0.8, 1]]))  # Return log-probability estimates for the test vector X.\n",
    "print(clf.score([[0.8, 1]],[1]))  # Returns the mean accuracy on the given test data and labels.\n",
    "print(clf.score([[0.8, 1]],[2]))  # Returns the mean accuracy on the given test data and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict function - Guess the set 1 or set 2. \n",
    "\n",
    "Class-Prior Value - Dataset density based on count set1 and set2\n",
    "\n",
    "Class-Count Value - Count set1 and set2\n",
    "\n",
    "Theta Value - Mean Value of each class\n",
    "\n",
    "Sigma Value - Variance of each class\n",
    "\n",
    "Score function - Shows that accuracy rate for given value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Function Result :  1 => Given value is in First Set.\n",
    "    \n",
    "Class Prior Value : 0.66 / 0.33 => In the set, Count of 1 : 4/6 Count of 2 : 2/6\n",
    "            \n",
    "Score Function : 1.0 / 0.0 => Given value: Is 0.8, 1 in [1] ? Yes given value is in set 1. TRUE 1.0\n",
    "\n",
    "And against to this => FALSE 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK-2: Please use the small dataset about gender classification with Gaussian Naive Bayes and compare the results.\n",
    "\n",
    "http://www.wikizero.biz/index.php?q=aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTmFpdmVfQmF5ZXNfY2xhc3NpZmllcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009975433349609375\n"
     ]
    }
   ],
   "source": [
    "## Height, Weight and Foot Size is input and we access to new variable in which set.\n",
    "## It has done Probability based estimation. This algorithm creates continuous function and trains.\n",
    "## Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "A = np.array([[6,180,12],[5.92,190,11],[5.58,170,12],[5.92,165,10],[5,100,6],[5.5,150,8],[5.42,130,7],[5.75,150,9]])\n",
    "B = np.array([1,1,1,1,2,2,2,2])\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB(priors=None)\n",
    "clf.fit(A, B) # GaussianNB(priors=None)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time difference : 0.00099"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below and above Cell values was taken by wikizero site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[0.5 0.5]\n",
      "[4. 4.]\n",
      "[[  5.855  176.25    11.25  ]\n",
      " [  5.4175 132.5      7.5   ]]\n",
      "[[2.62757340e-02 9.21875007e+01 6.87500734e-01]\n",
      " [7.29194840e-02 4.18750001e+02 1.25000073e+00]]\n",
      "[[1.93107041e-06 9.99998069e-01]]\n",
      "[[-1.31574361e+01 -1.93107228e-06]]\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([[5,150 ,10]]))\n",
    "print(clf.class_prior_)  # probability of each class.\n",
    "print(clf.class_count_)  # number of training samples observed in each class.\n",
    "print(clf.theta_)  # mean of each feature per class.\n",
    "print(clf.sigma_)  # variance of each feature per class\n",
    "print(clf.predict_proba([[5,150 ,10]]))  # Return probability estimates for the test vector X.\n",
    "print(clf.predict_log_proba([[5,150 ,10]]))  # Return log-probability estimates for the test vector X.\n",
    "print(clf.score([[6, 160 ,10]],[1]))  # Returns the mean accuracy on the given test data and labels.\n",
    "print(clf.score([[6, 160,10]],[2]))  # Returns the mean accuracy on the given test data and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Male Count : 4 and Female Count : 4 Class Prior -> 0.5 0.5\n",
    "        \n",
    "Predict Function Result -> [5,150,10] : Set [2] \n",
    "    \n",
    "[6,160,10] is in set [1] ? Yes , Result is TRUE. Against to it set [2] is FALSE. \n",
    "\n",
    "Match the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more small dataset and different from wikipage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009777545928955078\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "A = np.array([[5.67,150,11],[6,190,9],[5.48,150,12]])\n",
    "B = np.array([2,1,2])\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB(priors=None)\n",
    "clf.fit(A, B) # GaussianNB(priors=None)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the same algorithms run, time differences results are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[0.33333333 0.66666667]\n",
      "[1. 2.]\n",
      "[[  6.    190.      9.   ]\n",
      " [  5.575 150.     11.5  ]]\n",
      "[[3.55555556e-07 3.55555556e-07 3.55555556e-07]\n",
      " [9.02535556e-03 3.55555556e-07 2.50000356e-01]]\n",
      "[[0. 1.]]\n",
      "[[-2.25281247e+09  0.00000000e+00]]\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([[5,150 ,10]]))\n",
    "print(clf.class_prior_)  # probability of each class.\n",
    "print(clf.class_count_)  # number of training samples observed in each class.\n",
    "print(clf.theta_)  # mean of each feature per class.\n",
    "print(clf.sigma_)  # variance of each feature per class\n",
    "print(clf.predict_proba([[5,150 ,10]]))  # Return probability estimates for the test vector X.\n",
    "print(clf.predict_log_proba([[5,150 ,10]]))  # Return log-probability estimates for the test vector X.\n",
    "print(clf.score([[6, 160 ,10]],[1]))  # Returns the mean accuracy on the given test data and labels.\n",
    "print(clf.score([[6, 160,10]],[2]))  # Returns the mean accuracy on the given test data and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset is small, predict value was changed. And found false. \n",
    "So, dataset size is important we can see from this result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK-3: Please use the digits dataset with comparing Gaussian Naive Bayes and Support Vector Machine algorithms. \n",
    "\n",
    "Classification applications on the handwritten digits data \n",
    "In this example, you will see two different applications of Naive Bayesian Algorithm on the digits data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
      "       [ 0.,  0., 10., ..., 12.,  1.,  0.]]), 'target': array([0, 1, 2, ..., 8, 9, 8]), 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
      "        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
      "        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
      "        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
      "        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
      "        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
      "        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
      "        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
      "        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
      "        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
      "        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
      "        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
      "        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
      "        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
      "        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
      "        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]), 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}\n",
      "[[ 0.         -0.33501649 -0.04308102 ... -1.14664746 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...  0.54856067 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...  1.56568555  1.6951369\n",
      "  -0.19600752]\n",
      " ...\n",
      " [ 0.         -0.33501649 -0.88456568 ... -0.12952258 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -0.67419451 ...  0.8876023  -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649  1.00877481 ...  0.8876023  -0.26113572\n",
      "  -0.19600752]]\n",
      "[0 1 2 ... 8 9 8]\n",
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n",
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split  # some documents still include the cross-validation option but it no more exists in version 18.0\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pylab as plt\n",
    "\n",
    "########################################################################################################################\n",
    "##################################### PART A ###########################################################################\n",
    "########################################################################################################################\n",
    "np.random.seed(42)  # random seeding is performed\n",
    "digits = load_digits()  # the whole data set with the labels and other information are extracted\n",
    "data = scale(digits.data)  # the data is scaled with the use of z-score\n",
    "n_samples, n_features = data.shape  # the no. of samples and no. of features are determined with the help of shape\n",
    "n_digits = len(np.unique(digits.target))  # the number of labels are determined with the aid of unique formula\n",
    "labels = digits.target  # get the ground-truth labels into the labels\n",
    "\n",
    "print(digits)\n",
    "print(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n",
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n",
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(labels)  # the labels are printed on the screen\n",
    "print(digits.keys())  # this command will provide you the key elements in this dataset\n",
    "print(digits.DESCR)  # to get the descriptive information about this dataset\n",
    "\n",
    "pl.gray()  # turns an image into gray scale\n",
    "pl.matshow(digits.images[0])\n",
    "pl.show()\n",
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code , fit the normal digit data. Applied to GaussianNB algorithm and predict it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1257\n",
      "[[49  1  0  0  1  0  0  0  0  0]\n",
      " [ 0 53  1  0  0  0  0  0  3  0]\n",
      " [ 0  8 27  8  0  0  0  0 12  0]\n",
      " [ 0  0  0 51  0  0  0  1  3  1]\n",
      " [ 0  8  0  0 39  1  0  2  1  0]\n",
      " [ 0  0  0  1  0 46  0  3  1  0]\n",
      " [ 0  1  1  0  0  0 53  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 59  0  0]\n",
      " [ 0 13  0  1  0  3  0  2 31  0]\n",
      " [ 0  2  0  7  0  1  1  9  3 31]]\n",
      "0.812962962962963\n",
      "439\n",
      "540\n",
      "[0 1 2 3 4 5 6 7 8 9] [49 86 29 68 41 51 54 76 54 32]\n",
      "49\n",
      "{'priors': None, 'var_smoothing': 1e-09}\n",
      "[[1.00000000e+000 1.33287604e-061 1.60690901e-179 ... 2.49662423e-097\n",
      "  1.13601053e-128 8.46231091e-063]\n",
      " [0.00000000e+000 1.00000000e+000 2.76123832e-034 ... 0.00000000e+000\n",
      "  9.18988933e-019 2.58920112e-059]\n",
      " [0.00000000e+000 8.59467989e-001 1.23820991e-016 ... 0.00000000e+000\n",
      "  1.40532011e-001 3.90395127e-071]\n",
      " ...\n",
      " [0.00000000e+000 9.99992021e-001 5.29012737e-028 ... 0.00000000e+000\n",
      "  7.97861676e-006 1.21141946e-061]\n",
      " [0.00000000e+000 9.14745635e-002 2.27151003e-047 ... 0.00000000e+000\n",
      "  5.86793869e-019 9.08525437e-001]\n",
      " [0.00000000e+000 0.00000000e+000 3.35211718e-019 ... 0.00000000e+000\n",
      "  1.00000000e+000 5.34823739e-038]]\n"
     ]
    }
   ],
   "source": [
    "# Train-test split is one of the most critical issues in ML\n",
    "# Try this example with different test size like 0.1 or 0.5\n",
    "import time\n",
    "start = time.time()\n",
    "y = digits.target\n",
    "X = digits.data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "X1 = data\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y, test_size=0.3, random_state=10)\n",
    "\n",
    "print(len(X))\n",
    "print(len(X_train))\n",
    "\n",
    "\n",
    "gnb = GaussianNB(priors=None)\n",
    "# gnb = GaussianNB(priors=[0.1, 0.5, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05])\n",
    "fit = gnb.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "print(accuracy_score(y_test, predicted, normalize=False))  # the number of correct predictions\n",
    "print(len(predicted))  # the number of all of the predictions\n",
    "unique_p, counts_p = np.unique(predicted, return_counts=True)\n",
    "print(unique_p, counts_p)\n",
    "print((predicted == 0).sum())\n",
    "print(fit.get_params())\n",
    "print(fit.predict_proba(X))\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03692936897277832\n"
     ]
    }
   ],
   "source": [
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Difference : 0.036 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix => Rate of true predict for each color of digit array/image.\n",
    "Accuracy Rate => 0.81 . This result find by diagonal line values / Total Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, for the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "1257\n",
      "[[49  2  0  0  0  0  0  0  0  0]\n",
      " [ 0 55  0  0  0  0  0  0  2  0]\n",
      " [ 0 11 12 22  0  0  0  0 10  0]\n",
      " [ 0  0  0 51  0  0  0  1  3  1]\n",
      " [ 1 12  0  0 34  1  0  3  0  0]\n",
      " [ 0  0  0  1  0 46  0  3  1  0]\n",
      " [ 0  1  1  0  0  0 53  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 59  0  0]\n",
      " [ 0 20  0  1  0  3  0  2 24  0]\n",
      " [ 0  6  0  7  0  0  1  8  1 31]]\n",
      "0.7666666666666667\n",
      "414\n",
      "540\n",
      "[0 1 2 3 4 5 6 7 8 9] [ 50 107  13  82  35  50  54  76  41  32]\n",
      "50\n",
      "{'priors': None, 'var_smoothing': 1e-09}\n",
      "[[1.00000000e+000 4.77679989e-062 1.75037313e-189 ... 8.54617046e-100\n",
      "  1.36674852e-133 1.19344389e-066]\n",
      " [0.00000000e+000 1.00000000e+000 8.39292646e-044 ... 0.00000000e+000\n",
      "  3.08517769e-023 1.01890224e-062]\n",
      " [0.00000000e+000 9.99994511e-001 4.37897221e-026 ... 0.00000000e+000\n",
      "  5.48925974e-006 1.78747252e-074]\n",
      " ...\n",
      " [0.00000000e+000 1.00000000e+000 1.60797438e-037 ... 0.00000000e+000\n",
      "  2.67855766e-010 4.76721623e-065]\n",
      " [0.00000000e+000 9.96106778e-001 7.51842376e-056 ... 0.00000000e+000\n",
      "  2.14515347e-022 3.89322240e-003]\n",
      " [0.00000000e+000 0.00000000e+000 3.03500216e-024 ... 0.00000000e+000\n",
      "  9.99999992e-001 6.26912200e-037]]\n"
     ]
    }
   ],
   "source": [
    "# Train-test split is one of the most critical issues in ML\n",
    "# Try this example with different test size like 0.1 or 0.5\n",
    "y = digits.target\n",
    "X = digits.data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "X1 = data\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y, test_size=0.3, random_state=10)\n",
    "\n",
    "print(len(X))\n",
    "print(len(X_train))\n",
    "\n",
    "\n",
    "gnb = GaussianNB(priors=None)\n",
    "# gnb = GaussianNB(priors=[0.1, 0.5, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05])\n",
    "fit = gnb.fit(X_train1, y_train1)\n",
    "predicted = fit.predict(X_test1)\n",
    "print(confusion_matrix(y_test1, predicted))\n",
    "print(accuracy_score(y_test1, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "print(accuracy_score(y_test1, predicted, normalize=False))  # the number of correct predictions\n",
    "print(len(predicted))  # the number of all of the predictions\n",
    "unique_p, counts_p = np.unique(predicted, return_counts=True)\n",
    "print(unique_p, counts_p)\n",
    "print((predicted == 0).sum())\n",
    "print(fit.get_params())\n",
    "print(fit.predict_proba(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix => Rate of true predict for each color of digit array/image. Accuracy Rate => 0.76 . This result find by diagonal line values / Total Values\n",
    "\n",
    "Accuracy rate was decreased. Because dataset is not normal distributed. So, if the data was scaling, accuracy rate was descreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK - 4 Same dataset with SVM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
      "       [ 0.,  0., 10., ..., 12.,  1.,  0.]]), 'target': array([0, 1, 2, ..., 8, 9, 8]), 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
      "        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
      "        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
      "        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
      "        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
      "        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
      "        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
      "        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
      "        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
      "        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
      "        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
      "        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
      "        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
      "        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
      "        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
      "        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]), 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}\n",
      "[0 1 2 ... 8 9 8]\n",
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n",
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)  # random seeding is performed\n",
    "digits = load_digits()  # the whole data set with the labels and other information are extracted\n",
    "n_samples, n_features = data.shape  # the no. of samples and no. of features are determined with the help of shape\n",
    "n_digits = len(np.unique(digits.target))  # the number of labels are determined with the aid of unique formula\n",
    "labels = digits.target  # get the ground-truth labels into the labels\n",
    "\n",
    "print(digits)\n",
    "\n",
    "print(labels)  # the labels are printed on the screen\n",
    "print(digits.keys())  # this command will provide you the key elements in this dataset\n",
    "print(digits.DESCR)  # to get the descriptive information about this dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM algorithm seperate classes for each label. (According to their distances(Ocliddean Distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mercan.karacabey\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\mercan.karacabey\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  0  0  0  0 25  0  0  0  0]\n",
      " [ 0 15  0  0  0 42  0  0  0  0]\n",
      " [ 0  0 15  0  0 40  0  0  0  0]\n",
      " [ 0  0  0 24  0 32  0  0  0  0]\n",
      " [ 0  0  0  0 28 23  0  0  0  0]\n",
      " [ 0  0  0  0  0 51  0  0  0  0]\n",
      " [ 0  0  0  0  0 30 25  0  0  0]\n",
      " [ 0  0  0  0  0 48  0 12  0  0]\n",
      " [ 0  0  0  0  0 45  0  0  5  0]\n",
      " [ 0  0  0  0  0 42  0  0  0 12]]\n",
      "0.39444444444444443\n",
      "213\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "y = digits.target\n",
    "X = digits.data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "##X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "clf = SVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "fit = clf.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "print(accuracy_score(y_test, predicted, normalize=False))  # the number of correct predictions\n",
    "print(len(predicted))  # the number of all of the predictions\n",
    "unique_p, counts_p = np.unique(predicted, return_counts=True)\n",
    "\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma='auto', kernel='linear',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9745965003967285\n"
     ]
    }
   ],
   "source": [
    "print(elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
