{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION TO MACHINE LEARNING - ASSIGNMENT 2\n",
    "by Mercan Karacabey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QUESTION 1 : Please run the code for digit dataset that we have worked during the last class. Compare and discuss the outputs for the raw and scaled data as we did in the lab. Please assign the random state value randomly (you need to provide a random integer assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data preprocessing -- scaled data -- by z-score -- Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_digits: 10, \t n_samples 1797, \t n_features 64\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "digits_standardization = load_digits()\n",
    "## Data Scaling\n",
    "data = scale(digits_standardization.data)\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(digits_standardization.target))\n",
    "labels = digits_standardization.target\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=digits_standardization.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2     3     4     5    6    7    8    9  ...    54   55   56  \\\n",
       "0  0.0  0.0  5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0 ...   5.0  0.0  0.0   \n",
       "3  0.0  0.0  7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0 ...   9.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
       "\n",
       "    57   58    59    60    61   62   63  \n",
       "0  0.0  6.0  13.0  10.0   0.0  0.0  0.0  \n",
       "1  0.0  0.0  11.0  16.0  10.0  0.0  0.0  \n",
       "2  0.0  0.0   3.0  11.0  16.0  9.0  0.0  \n",
       "3  0.0  7.0  13.0  13.0   9.0  0.0  0.0  \n",
       "4  0.0  0.0   2.0  16.0   4.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# load the iris dataset\n",
    "np.random.seed(42)\n",
    "digits_normalization = load_digits()\n",
    "print(digits_normalization.data.shape)\n",
    "# separate the data from the target attributes\n",
    "X = digits_normalization.data\n",
    "y = digits_normalization.target\n",
    "# normalize the data attributes\n",
    "normalized_X = preprocessing.normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2: Please run the same code for 3 different test/train sizes such as 0.1, 0.3, 0.5 for raw dataset. Compare and discuss the obtained results. Which one is the best and why do you think so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Train Size : 0.1 - Row Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split  # some documents still include the cross-validation option but it no more exists in version 18.0\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = digits_standardization.target\n",
    "X = digits_standardization.data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=10)\n",
    "\n",
    "gnb = GaussianNB(priors=None)\n",
    "fit = gnb.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, predicted)) # the number of correct predictions\n",
    "unique_p, counts_p = np.unique(predicted, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Train Size : 0.3 - Row Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812962962962963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split  # some documents still include the cross-validation option but it no more exists in version 18.0\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = digits_standardization.target\n",
    "X = digits_standardization.data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "gnb = GaussianNB(priors=None)\n",
    "fit = gnb.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, predicted)) # the number of correct predictions\n",
    "unique_p, counts_p = np.unique(predicted, return_counts=True)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Train Size : 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8231368186874305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split  # some documents still include the cross-validation option but it no more exists in version 18.0\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = digits_standardization.target\n",
    "X = digits_standardization.data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=10)\n",
    "\n",
    "gnb = GaussianNB(priors=None)\n",
    "fit = gnb.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, predicted)) # the number of correct predictions\n",
    "unique_p, counts_p = np.unique(predicted, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3 : Please explain how the Gaussian Naive Bayesian model. How does Gaussian Naive Bayes algorithm work in the digit dataset? Assume that you are explaining it to a close friend of yours who has no background in data science but she is highly willing to learn it. Please do not forget to refer to the features, independence assumption, and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the assumption of the normal distribution, Gaussian Naive Bayes is best used in cases when all our features are continuous. The raw predicted probabilities from Gaussian naive Bayes (outputted using predict_proba) are not calibrated. That is, they should not be believed. If we want to create useful predicted probabilities we will need to calibrate them using an isotonic regression or a related method.\n",
    "\n",
    "Say you've label A and B (hidden)\n",
    "Label A\n",
    "    - Have multiple words with different probabilities\n",
    "    - Every word gives evidence if it's label A\n",
    "    - We mutiply all the probabilities with the prior to find the joint probability of A\n",
    "Label B\n",
    "    - Have multiple words with different probabilities\n",
    "    - Every word gives evidence if it's label B\n",
    "    - We multiply all the probabilities with the prior to find the joint probability of B\n",
    "Now you can find out the probability of it being A or B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 4 : Please run the SVC (linear and RBF) code for iris dataset (that we worked during the class). Please perform the same algorithm for 4 different C values that you will decide. Then you should plot the outputs in the 2 x 2 plots (you need to get rid of the plots for label propagation but instead use SVC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramerter C:\n",
    "Large Value of parameter C => small margin\n",
    "Small Value of paramerter C => Large margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C Value => 1.0 & Gamma Value => 0.1 (C Value Const - Changed only Gamma Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import svm\n",
    "\n",
    "np.random.seed(42)\n",
    "iris = load_iris()\n",
    "\n",
    "y = iris.target\n",
    "X = iris.data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "clf = svm.SVC(kernel='linear', C = 1.0, gamma = 0.1)\n",
    "\n",
    "fit = clf.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C Value => 1.0 & Gamma Value => 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import svm\n",
    "\n",
    "np.random.seed(42)\n",
    "iris = load_iris()\n",
    "\n",
    "y = iris.target\n",
    "X = iris.data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "clf = svm.SVC(kernel='linear', C = 1.0, gamma=1)\n",
    "\n",
    "fit = clf.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C Value => 1.0 & Gamma Value => 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import svm\n",
    "\n",
    "np.random.seed(42)\n",
    "iris = load_iris()\n",
    "\n",
    "y = iris.target\n",
    "X = iris.data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "clf = svm.SVC(kernel='linear', C = 1.0, gamma=100)\n",
    "\n",
    "fit = clf.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C Value => 1.0 & Gamma Value => 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import svm\n",
    "\n",
    "np.random.seed(42)\n",
    "iris = load_iris()\n",
    "\n",
    "y = iris.target\n",
    "X = iris.data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "clf = svm.SVC(kernel='linear', C = 1.0, gamma= 1000)\n",
    "\n",
    "fit = clf.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C Value => 100.0 & Gamma Value => 1 (C Value Changed & Gamma Value Const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import svm\n",
    "\n",
    "np.random.seed(42)\n",
    "iris = load_iris()\n",
    "\n",
    "y = iris.target\n",
    "X = iris.data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "clf = svm.SVC(kernel='linear', C = 100.0, gamma= 1)\n",
    "\n",
    "fit = clf.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C Value => 1000.0 & Gamma Value => 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import svm\n",
    "\n",
    "np.random.seed(42)\n",
    "iris = load_iris()\n",
    "\n",
    "y = iris.target\n",
    "X = iris.data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "clf = svm.SVC(kernel='linear', C = 1000.0, gamma= 1.0)\n",
    "\n",
    "fit = clf.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C value Changed & Gamma Value Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import svm\n",
    "\n",
    "np.random.seed(42)\n",
    "iris = load_iris()\n",
    "\n",
    "y = iris.target\n",
    "X = iris.data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "clf = svm.SVC(kernel='linear', C = 10000.0, gamma= 1.0)\n",
    "\n",
    "fit = clf.fit(X_train, y_train)\n",
    "predicted = fit.predict(X_test)\n",
    "print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Â Question 6: Please explain the need for semi-supervised learning methods. Please try to find a concrete example and discuss it. (At most 70 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised classification: Labeled data is used to help identify that there are specific groups of webpage types present in the data and what they might be. The algorithm is then trained on unlabeled data to define the boundaries of those webpage types and may even identify new types of webpages that were unspecified in the existing human-inputted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 7 : Please provide two concrete examples: one of them is more suitable for applying NaÃ¯ve Bayes algorithm and the other for Support Vector Machine algorithm. Provide a very rough comparison accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Algorithms -> A school with a total population of 100 persons. These 100 persons can be seen either as â€˜Studentsâ€™ and â€˜Teachersâ€™ or as a population of â€˜Malesâ€™ and â€˜Femalesâ€™.\n",
    "\n",
    "100 people, what is the conditional probability that a certain member of the school is a â€˜Teacherâ€™ given that he is a â€˜Manâ€™?\n",
    "\n",
    "Support Vector Machine -> Detect (localize) standing humans in an image \n",
    "Face detection with a sliding window classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
